{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516b97af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 11. Técnicas de Seleção de Modelos\n",
    "\n",
    "#### 11.1. Importância da seleção de modelos\n",
    "A seleção de modelos é crucial em machine learning porque permite escolher o modelo que melhor se adapta aos dados e ao problema específico. A escolha do modelo certo pode melhorar significativamente a performance e a generalização do modelo.\n",
    "\n",
    "#### 11.2. Divisão de Dados: Treino, Validação e Teste\n",
    "Dividir os dados em conjuntos de treino, validação e teste é fundamental para avaliar a performance dos modelos de forma justa e para evitar overfitting.\n",
    "\n",
    "##### 11.2.1. Divisão simples em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cacc8fd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 5) (20, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de dados\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Divisão em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d54dc49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### 11.2.2. Divisão com Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c8f4930",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 5) (20, 5) (20, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de dados\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Divisão em treino, validação e teste\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b76950",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 11.3. Técnicas de Validação Cruzada\n",
    "A validação cruzada é uma técnica robusta para avaliar a performance do modelo e garantir que ele generalize bem.\n",
    "\n",
    "##### 11.3.1. K-Fold Cross Validation\n",
    "Divide os dados em k subconjuntos (folds) e realiza o treinamento e validação k vezes, usando um fold diferente para validação a cada vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "97c2a9c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7  0.45 0.55 0.6  0.55]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de dados\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Aplicar K-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=kf)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c180b99",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### 11.3.2. Stratified K-Fold Cross Validation\n",
    "Similar ao K-Fold, mas mantém a proporção das classes em cada fold, útil para dados desbalanceados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a9dd6e67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6  0.4  0.45 0.5  0.6 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de dados\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Aplicar Stratified K-Fold Cross Validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=skf)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc9244",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 11.4. Grid Search e Random Search\n",
    "Técnicas para encontrar os melhores hiperparâmetros para um modelo.\n",
    "\n",
    "##### 11.4.1. Grid Search\n",
    "Explora todas as combinações possíveis de hiperparâmetros em uma grade predefinida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6642328c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de dados\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Definir parâmetros para Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Aplicar Grid Search\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f660cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### 11.4.2. Random Search\n",
    "Explora uma amostra aleatória das combinações possíveis de hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04347d0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 10, 'min_samples_split': 5, 'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de dados\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Definir parâmetros para Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Aplicar Random Search\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b9110",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 11.5. Avaliação de Modelos\n",
    "Avaliar a performance dos modelos utilizando métricas apropriadas para o problema em questão.\n",
    "\n",
    "##### 11.5.1. Acurácia, Precisão, Recall e F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5a192e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.8\n",
      "Precisão: 0.8\n",
      "Recall: 0.8\n",
      "F1-Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de predições\n",
    "y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 0, 0, 0, 1, 0, 1, 1, 1])\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Acurácia: {accuracy}\")\n",
    "print(f\"Precisão: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca200240",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "matplotlib\n",
    "imblearn\n",
    "sklearn\n",
    "patsy\n",
    "numpy\n",
    "gensim\n",
    "pandas\n",
    "nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
