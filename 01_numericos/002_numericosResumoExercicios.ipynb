{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Manipular dados numéricos para algoritmos de Machine Learning\n",
    "\n",
    "A.1. Explicar o que é uma feature e a sua relação com os modelos de machine learning\n",
    "\n",
    "**Definição:** \n",
    "Uma feature, ou característica, é uma variável de entrada usada por um modelo de machine learning para fazer previsões ou tomar decisões. As features são essenciais porque carregam a informação a partir da qual o modelo aprende. Em termos mais práticos, as features podem ser vistas como colunas em uma tabela de dados, onde cada coluna representa um tipo específico de informação (como altura, peso, idade, etc.) que pode ser usada para prever um resultado (como o risco de uma doença, a classificação de um documento, etc.). A escolha e a qualidade das features influenciam diretamente o desempenho do modelo.\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Crie um dataset fictício com pelo menos 5 features (colunas) e 100 exemplos (linhas).\n",
    "2. Escolha uma variável alvo e descreva como cada feature pode potencialmente influenciar a variável alvo.\n",
    "3. Utilize um algoritmo de regressão linear para treinar um modelo utilizando as features e avalie a performance.\n",
    "\n",
    "A.2. Explicar o que são escalares, vetores e espaços\n",
    "\n",
    "**Definição:**\n",
    "- **Escalares:** Um escalar é um único número real ou complexo. Em machine learning, um escalar pode representar uma única medida ou valor, como a temperatura em um dado dia.\n",
    "- **Vetores:** Um vetor é uma sequência de números (escalares) dispostos em uma determinada ordem. Em machine learning, vetores são frequentemente usados para representar exemplos de dados. Por exemplo, um vetor pode representar as características (features) de um paciente, como [idade, peso, pressão arterial].\n",
    "- **Espaços:** Um espaço, especificamente um espaço vetorial, é um conjunto de vetores que podem ser adicionados uns aos outros e multiplicados por escalares para formar novos vetores dentro do mesmo espaço. Em machine learning, o espaço vetorial das features é onde as operações de modelagem acontecem. Este espaço pode ser de alta dimensão dependendo do número de features usadas.\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Crie uma lista de 10 números aleatórios (escalars).\n",
    "2. Transforme essa lista em um vetor.\n",
    "3. Adicione dois vetores e multiplique um vetor por um escalar.\n",
    "4. Visualize os vetores em um gráfico (se forem de 2 ou 3 dimensões) para melhor compreensão do espaço vetorial.\n",
    "\n",
    "A.3. Realizar discretização de variáveis contínuas por meio de quantização com bins fixos\n",
    "\n",
    "**Definição:**\n",
    "A discretização de variáveis contínuas é o processo de converter variáveis contínuas em variáveis discretas dividindo o intervalo contínuo em partes distintas chamadas \"bins\". Isso pode ser feito de várias maneiras, uma das quais é utilizando bins de largura fixa.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bin 4', 'Bin 10', 'Bin 2', 'Bin 7', 'Bin 6', ..., 'Bin 9', 'Bin 3', 'Bin 10', 'Bin 2', 'Bin 3']\n",
      "Length: 100\n",
      "Categories (10, object): ['Bin 1' < 'Bin 2' < 'Bin 3' < 'Bin 4' ... 'Bin 7' < 'Bin 8' < 'Bin 9' < 'Bin 10']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100) * 100  # 100 valores aleatórios entre 0 e 100\n",
    "\n",
    "# Discretização com bins de largura fixa\n",
    "bins = np.linspace(0, 100, 11)  # 10 bins de largura 10\n",
    "labels = [f'Bin {i}' for i in range(1, len(bins))]\n",
    "dados_discretizados = pd.cut(dados, bins=bins, labels=labels)\n",
    "\n",
    "print(dados_discretizados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Gere um conjunto de 200 valores contínuos aleatórios.\n",
    "2. Aplique a discretização com bins de largura fixa de modo que os valores sejam divididos em 5 categorias.\n",
    "3. Plote um histograma dos valores antes e depois da discretização.\n",
    "\n",
    "A.4. Realizar discretização de variáveis contínuas por meio de quantização com bins variáveis\n",
    "\n",
    "**Definição:**\n",
    "A discretização com bins variáveis, ou \"quantiles\", divide os dados em intervalos de tal forma que cada bin contém aproximadamente o mesmo número de pontos de dados.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 7 9 5 7 2 2 4 1 4 6 4 9 6 5 1 6 3 6 3 8 2 9 7 1 4 3 6 0 8 4 1 0 2 5 8 4\n",
      " 8 0 8 5 9 4 4 2 3 3 8 9 7 2 7 6 7 1 5 3 4 8 0 2 1 6 9 2 5 3 5 6 1 0 8 9 8\n",
      " 0 6 6 1 7 0 0 5 5 0 5 0 9 4 3 1 2 9 7 7 3 2 7 1 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100) * 100  # 100 valores aleatórios entre 0 e 100\n",
    "\n",
    "# Discretização com quantiles\n",
    "dados_discretizados = pd.qcut(dados, q=10, labels=False)\n",
    "\n",
    "print(dados_discretizados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize o mesmo conjunto de dados do exercício anterior.\n",
    "2. Aplique a discretização utilizando quantiles de forma que os dados sejam divididos em 4 categorias.\n",
    "3. Compare os resultados da discretização com bins fixos e quantiles utilizando gráficos.\n",
    "\n",
    "A.5. Utilizar a FunctionTransformer do sklearn em uma variável\n",
    "\n",
    "**Definição:**\n",
    "A `FunctionTransformer` permite transformar os dados aplicando uma função customizada. Pode ser útil para aplicar transformações que não estão diretamente disponíveis nas outras classes de transformação do scikit-learn.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4]\n",
      " [ 9 16]\n",
      " [25 36]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Função customizada: quadrado dos valores\n",
    "transformer = FunctionTransformer(np.square)\n",
    "dados_transformados = transformer.transform(dados)\n",
    "\n",
    "print(dados_transformados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados com 3 features e 50 exemplos.\n",
    "2. Aplique uma função personalizada de transformação (exemplo: logaritmo) utilizando `FunctionTransformer`.\n",
    "3. Verifique o resultado e plote os dados antes e depois da transformação.\n",
    "\n",
    "A.6. Utilizar a PowerTransformer do sklearn em uma variável\n",
    "\n",
    "**Definição:**\n",
    "A `PowerTransformer` aplica uma transformação de potência (Box-Cox ou Yeo-Johnson) para estabilizar a variância e tornar os dados mais gaussianos.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.45653675e+00]\n",
      " [-1.82435019e+00]\n",
      " [-9.74997502e-01]\n",
      " [-2.72559394e-01]\n",
      " [-6.45741234e-01]\n",
      " [ 1.29190322e+00]\n",
      " [-9.39048694e-02]\n",
      " [ 1.15238891e+00]\n",
      " [ 1.36291075e+00]\n",
      " [ 9.36167203e-01]\n",
      " [-3.74662199e-01]\n",
      " [-1.03632417e+00]\n",
      " [-1.42629302e+00]\n",
      " [-6.53198590e-01]\n",
      " [ 1.22477183e+00]\n",
      " [ 1.37947660e+00]\n",
      " [-1.24147814e+00]\n",
      " [ 1.23635477e+00]\n",
      " [-6.76369310e-01]\n",
      " [ 7.39119092e-01]\n",
      " [ 7.77026622e-01]\n",
      " [-1.27181005e+00]\n",
      " [-1.53748721e+00]\n",
      " [ 4.76657682e-01]\n",
      " [-1.06960942e+00]\n",
      " [ 5.83230891e-01]\n",
      " [-2.05406383e+00]\n",
      " [-1.00435901e+00]\n",
      " [ 4.97739326e-02]\n",
      " [-8.67619049e-01]\n",
      " [-4.82841095e-01]\n",
      " [-1.00079808e+00]\n",
      " [ 7.47636250e-01]\n",
      " [ 8.20980686e-01]\n",
      " [ 1.40522330e+00]\n",
      " [-2.01638121e-01]\n",
      " [-1.22146410e+00]\n",
      " [-3.94797066e-03]\n",
      " [ 2.58273465e-01]\n",
      " [ 8.60826086e-01]\n",
      " [ 2.80470006e-01]\n",
      " [-1.63284638e+00]\n",
      " [ 4.68326496e-01]\n",
      " [-9.93022340e-01]\n",
      " [ 1.31821466e-01]\n",
      " [ 2.85506796e-01]\n",
      " [ 3.50278650e-01]\n",
      " [ 1.19718627e+00]\n",
      " [ 1.36014655e+00]\n",
      " [ 7.06070353e-01]\n",
      " [ 4.26983477e-01]\n",
      " [ 8.04301224e-01]\n",
      " [ 1.35338830e+00]\n",
      " [ 9.31359188e-01]\n",
      " [-3.11495060e-01]\n",
      " [-1.41276389e+00]\n",
      " [ 1.40211416e+00]\n",
      " [ 1.30729756e-01]\n",
      " [ 1.22038881e+00]\n",
      " [-1.37555012e+00]\n",
      " [-3.99766982e-01]\n",
      " [ 1.32190526e+00]\n",
      " [-1.17982977e+00]\n",
      " [-8.06385962e-01]\n",
      " [ 1.46271661e+00]\n",
      " [ 4.58631010e-01]\n",
      " [-1.28684680e+00]\n",
      " [ 6.01519965e-01]\n",
      " [-5.00161088e-01]\n",
      " [-1.00037435e-01]\n",
      " [-1.36279337e-03]\n",
      " [-1.76513971e-01]\n",
      " [-1.75021903e+00]\n",
      " [-1.65476607e+00]\n",
      " [ 5.45425416e-01]\n",
      " [ 2.08743507e-01]\n",
      " [-1.87026929e+00]\n",
      " [ 9.55641062e-01]\n",
      " [ 1.44817751e+00]\n",
      " [ 9.77772454e-01]\n",
      " [-5.80241977e-01]\n",
      " [-7.32170813e-01]\n",
      " [-7.08490433e-01]\n",
      " [-7.60133035e-01]\n",
      " [ 1.02613446e-01]\n",
      " [ 1.97114596e-02]\n",
      " [-1.32549142e+00]\n",
      " [ 6.87587282e-01]\n",
      " [-2.06407371e+00]\n",
      " [ 7.90289017e-01]\n",
      " [ 6.77470870e-01]\n",
      " [ 4.77849295e-01]\n",
      " [ 7.86731970e-01]\n",
      " [ 7.80962236e-01]\n",
      " [ 1.44312714e+00]\n",
      " [ 2.01609772e-01]\n",
      " [-8.51873305e-01]\n",
      " [ 3.47434812e-01]\n",
      " [ 1.14995789e+00]\n",
      " [-8.44379308e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 1) * 100  # 100 valores aleatórios entre 0 e 100\n",
    "\n",
    "# Aplicar PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "dados_transformados = pt.fit_transform(dados)\n",
    "\n",
    "print(dados_transformados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Gere um conjunto de dados com distribuição não normal.\n",
    "2. Aplique a transformação `PowerTransformer` e verifique a distribuição resultante.\n",
    "3. Compare a distribuição dos dados antes e depois da transformação utilizando gráficos.\n",
    "\n",
    "A.7. Utilizar a normalização Min-Max do sklearn para garantir que os dados estão na mesma faixa-dinâmica\n",
    "\n",
    "**Definição:**\n",
    "A normalização Min-Max escala os dados para que todos os valores fiquem dentro de um intervalo especificado, geralmente [0, 1].\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29698611 0.10131713]\n",
      " [0.84218935 0.58740665]\n",
      " [0.01393669 0.67787934]\n",
      " [0.91983724 0.22772782]\n",
      " [0.33325783 0.91391518]\n",
      " [0.22139522 0.88321287]\n",
      " [0.89802505 0.79096503]\n",
      " [0.01739458 0.        ]\n",
      " [0.03962041 0.91855289]\n",
      " [0.39311509 0.81157749]\n",
      " [0.85543406 0.37412604]\n",
      " [0.01095799 0.64591742]\n",
      " [0.72961907 0.90522403]\n",
      " [0.413428   1.        ]\n",
      " [1.         0.55101298]\n",
      " [0.10129934 0.82613002]\n",
      " [0.76817836 0.59267065]\n",
      " [0.72132348 0.57389458]\n",
      " [0.13408622 0.34371317]\n",
      " [0.56792612 0.43821496]\n",
      " [0.17762362 0.55813523]\n",
      " [0.42666124 0.4249088 ]\n",
      " [0.63752153 0.33122103]\n",
      " [0.67129077 0.07545131]\n",
      " [0.74561449 0.880712  ]\n",
      " [0.43290372 0.35923663]\n",
      " [0.25584839 0.29364086]\n",
      " [0.25340281 0.90987125]\n",
      " [0.36597624 0.13732499]\n",
      " [0.08809685 0.19048659]\n",
      " [0.73687043 0.77375753]\n",
      " [0.11516257 0.66381707]\n",
      " [0.5901634  0.76553509]\n",
      " [0.41150813 0.43221633]\n",
      " [0.94858418 0.18344251]\n",
      " [0.82053708 0.36083571]\n",
      " [0.49786864 0.78141805]\n",
      " [0.19462497 0.70113357]\n",
      " [0.3625377  0.43515021]\n",
      " [0.11148282 0.35766022]\n",
      " [0.11603048 0.87149774]\n",
      " [0.7946242  0.15058913]\n",
      " [0.69279654 0.2575405 ]\n",
      " [0.63504742 0.36419826]\n",
      " [0.32253201 0.42433971]\n",
      " [0.4974009  0.96285025]\n",
      " [0.99806406 0.7496771 ]\n",
      " [0.25354546 0.13246796]\n",
      " [0.49017893 0.48208973]\n",
      " [0.50171932 0.74222989]\n",
      " [0.11574053 0.66870485]\n",
      " [0.00210912 0.27384318]\n",
      " [0.72400899 0.33419093]\n",
      " [0.03326635 0.88842965]\n",
      " [0.60762764 0.79562949]\n",
      " [0.15814944 0.13049691]\n",
      " [0.71688618 0.15127378]\n",
      " [0.64798334 0.78591639]\n",
      " [0.4545661  0.22814665]\n",
      " [0.55586486 0.12881639]\n",
      " [0.40839029 0.92530296]\n",
      " [0.05537942 0.35875766]\n",
      " [0.69175155 0.58052196]\n",
      " [0.35452997 0.19435904]\n",
      " [0.77849443 0.21197514]\n",
      " [0.76567046 0.65394191]\n",
      " [0.6640164  0.5043885 ]\n",
      " [0.25925476 0.7115677 ]\n",
      " [0.68865263 0.11275914]\n",
      " [0.03884191 0.3874166 ]\n",
      " [0.70254594 0.77335622]\n",
      " [0.59208947 0.0934804 ]\n",
      " [0.91428805 0.92951237]\n",
      " [0.32379482 0.91891717]\n",
      " [0.03469542 0.28266523]\n",
      " [0.         0.25404203]\n",
      " [0.89028194 0.19287725]\n",
      " [0.16016555 0.47482673]\n",
      " [0.27318685 0.46103359]\n",
      " [0.7967984  0.13638044]\n",
      " [0.66998324 0.09304372]\n",
      " [0.44593894 0.48858734]\n",
      " [0.52100455 0.69887666]\n",
      " [0.3037392  0.31930033]\n",
      " [0.29113798 0.81906378]\n",
      " [0.35865279 0.5470709 ]\n",
      " [0.86778699 0.52082188]\n",
      " [0.12828511 0.86930735]\n",
      " [0.00332453 0.87701926]\n",
      " [0.86114487 0.7036599 ]\n",
      " [0.51401609 0.66651521]\n",
      " [0.58204712 0.03669333]\n",
      " [0.88821079 0.47733851]\n",
      " [0.91321684 0.10497696]\n",
      " [0.41497232 0.95287573]\n",
      " [0.78387776 0.85394096]\n",
      " [0.39502925 0.64937597]\n",
      " [0.97623964 0.96840663]\n",
      " [0.25138504 0.03637112]\n",
      " [0.21960696 0.66330129]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 2) * 100  # 100 valores aleatórios entre 0 e 100\n",
    "\n",
    "# Aplicar Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "dados_normalizados = scaler.fit_transform(dados)\n",
    "\n",
    "print(dados_normalizados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados com 5 features e 200 exemplos.\n",
    "2. Aplique a normalização Min-Max para garantir que todos os dados estão na faixa [0, 1].\n",
    "3. Plote um gráfico de cada feature antes e depois da normalização.\n",
    "\n",
    "A.8. Utilizar a normalização Standard Scaler\n",
    "\n",
    "**Definição:**\n",
    "A normalização com `StandardScaler` remove a média e escala os dados para que tenham desvio padrão unitário.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.26517031 -1.39901454]\n",
      " [ 0.69799621 -1.15064596]\n",
      " [-0.28488611  0.38551224]\n",
      " [-0.28160839 -1.50587895]\n",
      " [-0.25203402 -1.38391832]\n",
      " [ 1.30016212 -0.93320835]\n",
      " [ 0.61741627  0.53526383]\n",
      " [ 0.34362279  0.52013077]\n",
      " [ 0.17258852 -0.57604318]\n",
      " [-1.19330841 -1.09294466]\n",
      " [-0.7316898   0.65618369]\n",
      " [-1.67301997 -1.20708213]\n",
      " [ 1.65663452  0.71144846]\n",
      " [ 0.38648702  1.71268979]\n",
      " [-0.81371849 -0.68329061]\n",
      " [ 1.81953518  1.80997868]\n",
      " [-0.13677057  1.43581202]\n",
      " [-0.44860312  0.53591838]\n",
      " [ 1.44712613  0.0714877 ]\n",
      " [-0.05006525 -0.77449562]\n",
      " [-0.19458469  0.05655599]\n",
      " [-0.47875935  1.43705797]\n",
      " [-0.63467732  1.47242839]\n",
      " [-0.80267103  0.76965657]\n",
      " [-0.5628952  -1.25965895]\n",
      " [ 1.4435346  -0.43340833]\n",
      " [ 0.670104   -0.28474601]\n",
      " [-0.17187753 -1.2479676 ]\n",
      " [ 0.47271183 -0.6835328 ]\n",
      " [-1.09593668  1.21243013]\n",
      " [-0.81934376  1.65078761]\n",
      " [ 0.45928259 -0.3540111 ]\n",
      " [-0.97978055 -1.29777272]\n",
      " [ 1.51274945 -0.98830472]\n",
      " [-1.35914474 -1.15653657]\n",
      " [ 1.58649522 -1.45642711]\n",
      " [ 1.08215761  0.34169941]\n",
      " [-0.85831103  0.99457205]\n",
      " [-0.16720023 -0.95075904]\n",
      " [ 1.6815338   1.05206487]\n",
      " [-0.43836885 -1.66729484]\n",
      " [-0.7916313  -0.00380213]\n",
      " [-0.43180962  0.15074996]\n",
      " [-0.04995828  0.27795545]\n",
      " [-1.49050665  0.4725162 ]\n",
      " [-1.1956733   1.42806224]\n",
      " [ 0.91504112  0.23964102]\n",
      " [ 0.43006721 -1.25722632]\n",
      " [ 1.00596736 -0.28331634]\n",
      " [ 1.67727553  1.0854503 ]\n",
      " [-0.96157832  0.46455191]\n",
      " [ 0.01470302 -0.11440544]\n",
      " [-0.00638204  1.53830223]\n",
      " [-0.73111319 -1.46871904]\n",
      " [ 0.63072491  1.45192956]\n",
      " [ 1.39522545  0.45807882]\n",
      " [-0.95181029 -0.18646111]\n",
      " [-0.81240554  1.56256965]\n",
      " [ 0.52040959  0.86048656]\n",
      " [-1.30526872  0.23673239]\n",
      " [ 0.19963374 -0.73067365]\n",
      " [-1.27219117  0.06599761]\n",
      " [ 1.03701667  0.65453034]\n",
      " [ 0.53157672 -1.18350665]\n",
      " [-0.00398747 -0.01828876]\n",
      " [-1.36719183  0.86979774]\n",
      " [ 0.7900986   0.66591843]\n",
      " [ 0.98759898 -0.96854278]\n",
      " [ 0.84121462  0.10502896]\n",
      " [ 0.07476992 -1.2493181 ]\n",
      " [ 1.72817974  1.32079407]\n",
      " [-1.71516199 -1.54860058]\n",
      " [-1.64509067 -1.61221562]\n",
      " [ 1.21005363 -0.56695336]\n",
      " [ 1.69448277  1.50271699]\n",
      " [ 0.0026793   1.64812196]\n",
      " [ 1.0463295  -0.49049377]\n",
      " [-0.92074903 -0.75454944]\n",
      " [-0.9830325  -0.04120694]\n",
      " [ 0.21123711  1.57988912]\n",
      " [ 0.53320339 -1.45536   ]\n",
      " [-1.53457172  1.54099447]\n",
      " [-1.58816753  1.47715773]\n",
      " [-1.37986135 -0.4410511 ]\n",
      " [ 0.84423451  1.19195476]\n",
      " [-1.47631007 -0.54941177]\n",
      " [ 0.29889251 -0.87897185]\n",
      " [-0.66098692 -0.26038387]\n",
      " [ 0.28459174 -1.18848108]\n",
      " [-0.30429427 -0.27491012]\n",
      " [ 0.18823207  0.46138951]\n",
      " [-1.41434396 -1.29193165]\n",
      " [-0.06339047 -0.01051793]\n",
      " [ 0.6236852  -0.1576744 ]\n",
      " [ 0.8958625   0.03596999]\n",
      " [-1.3525256   0.87499195]\n",
      " [ 0.99254726 -0.32887776]\n",
      " [ 1.73970565 -0.74801723]\n",
      " [ 1.69325823 -0.06830893]\n",
      " [-0.28421716  1.03516136]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 2) * 100  # 100 valores aleatórios entre 0 e 100\n",
    "\n",
    "# Aplicar Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "dados_normalizados = scaler.fit_transform(dados)\n",
    "\n",
    "print(dados_normalizados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados com 4 features e 150 exemplos.\n",
    "2. Aplique a normalização `StandardScaler`.\n",
    "3. Verifique a média e o desvio padrão dos dados antes e depois da normalização.\n",
    "\n",
    "A.9. Utilizar a regularização norma-l2\n",
    "\n",
    "**Definição:**\n",
    "A regularização L2, também conhecida como regularização de Ridge, adiciona uma penalidade igual ao quadrado da magnitude dos coeficientes ao objetivo de otimização.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03732184 -0.06559702]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Dados de exemplo\n",
    "X = np.random.rand(100, 2)  # 100 exemplos, 2 features\n",
    "y = np.random.rand(100)  # 100 valores alvo\n",
    "\n",
    "# Aplicar Ridge Regression com regularização L2\n",
    "modelo = Ridge(alpha=1.0)\n",
    "modelo.fit(X, y)\n",
    "\n",
    "print(modelo.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Gere um conjunto de dados com 3 features e 100 exemplos.\n",
    "2. Aplique a regularização L2 utilizando `Ridge`.\n",
    "3. Compare os coeficientes do modelo com e sem regularização.\n",
    "\n",
    "A.10. Selecionar as features úteis para o modelo usando uma das três\n",
    "\n",
    " técnicas: Filtragem, Wrapper e Embedding\n",
    "\n",
    "**Definição:**\n",
    "- **Filtragem:** Seleciona as features com base em estatísticas univariadas, como a correlação.\n",
    "- **Wrapper:** Usa um modelo de machine learning para avaliar a importância de cada subset de features.\n",
    "- **Embedding:** As features são selecionadas durante o treinamento do modelo, como nos modelos baseados em árvores.\n",
    "\n",
    "**Exemplo em Python (Filtragem):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.7 0.4]\n",
      " [1.4 0.3]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.1]\n",
      " [1.5 0.2]\n",
      " [1.6 0.2]\n",
      " [1.4 0.1]\n",
      " [1.1 0.1]\n",
      " [1.2 0.2]\n",
      " [1.5 0.4]\n",
      " [1.3 0.4]\n",
      " [1.4 0.3]\n",
      " [1.7 0.3]\n",
      " [1.5 0.3]\n",
      " [1.7 0.2]\n",
      " [1.5 0.4]\n",
      " [1.  0.2]\n",
      " [1.7 0.5]\n",
      " [1.9 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.4]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.2]\n",
      " [1.5 0.4]\n",
      " [1.5 0.1]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.2 0.2]\n",
      " [1.3 0.2]\n",
      " [1.4 0.1]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.3 0.3]\n",
      " [1.3 0.3]\n",
      " [1.3 0.2]\n",
      " [1.6 0.6]\n",
      " [1.9 0.4]\n",
      " [1.4 0.3]\n",
      " [1.6 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [4.7 1.4]\n",
      " [4.5 1.5]\n",
      " [4.9 1.5]\n",
      " [4.  1.3]\n",
      " [4.6 1.5]\n",
      " [4.5 1.3]\n",
      " [4.7 1.6]\n",
      " [3.3 1. ]\n",
      " [4.6 1.3]\n",
      " [3.9 1.4]\n",
      " [3.5 1. ]\n",
      " [4.2 1.5]\n",
      " [4.  1. ]\n",
      " [4.7 1.4]\n",
      " [3.6 1.3]\n",
      " [4.4 1.4]\n",
      " [4.5 1.5]\n",
      " [4.1 1. ]\n",
      " [4.5 1.5]\n",
      " [3.9 1.1]\n",
      " [4.8 1.8]\n",
      " [4.  1.3]\n",
      " [4.9 1.5]\n",
      " [4.7 1.2]\n",
      " [4.3 1.3]\n",
      " [4.4 1.4]\n",
      " [4.8 1.4]\n",
      " [5.  1.7]\n",
      " [4.5 1.5]\n",
      " [3.5 1. ]\n",
      " [3.8 1.1]\n",
      " [3.7 1. ]\n",
      " [3.9 1.2]\n",
      " [5.1 1.6]\n",
      " [4.5 1.5]\n",
      " [4.5 1.6]\n",
      " [4.7 1.5]\n",
      " [4.4 1.3]\n",
      " [4.1 1.3]\n",
      " [4.  1.3]\n",
      " [4.4 1.2]\n",
      " [4.6 1.4]\n",
      " [4.  1.2]\n",
      " [3.3 1. ]\n",
      " [4.2 1.3]\n",
      " [4.2 1.2]\n",
      " [4.2 1.3]\n",
      " [4.3 1.3]\n",
      " [3.  1.1]\n",
      " [4.1 1.3]\n",
      " [6.  2.5]\n",
      " [5.1 1.9]\n",
      " [5.9 2.1]\n",
      " [5.6 1.8]\n",
      " [5.8 2.2]\n",
      " [6.6 2.1]\n",
      " [4.5 1.7]\n",
      " [6.3 1.8]\n",
      " [5.8 1.8]\n",
      " [6.1 2.5]\n",
      " [5.1 2. ]\n",
      " [5.3 1.9]\n",
      " [5.5 2.1]\n",
      " [5.  2. ]\n",
      " [5.1 2.4]\n",
      " [5.3 2.3]\n",
      " [5.5 1.8]\n",
      " [6.7 2.2]\n",
      " [6.9 2.3]\n",
      " [5.  1.5]\n",
      " [5.7 2.3]\n",
      " [4.9 2. ]\n",
      " [6.7 2. ]\n",
      " [4.9 1.8]\n",
      " [5.7 2.1]\n",
      " [6.  1.8]\n",
      " [4.8 1.8]\n",
      " [4.9 1.8]\n",
      " [5.6 2.1]\n",
      " [5.8 1.6]\n",
      " [6.1 1.9]\n",
      " [6.4 2. ]\n",
      " [5.6 2.2]\n",
      " [5.1 1.5]\n",
      " [5.6 1.4]\n",
      " [6.1 2.3]\n",
      " [5.6 2.4]\n",
      " [5.5 1.8]\n",
      " [4.8 1.8]\n",
      " [5.4 2.1]\n",
      " [5.6 2.4]\n",
      " [5.1 2.3]\n",
      " [5.1 1.9]\n",
      " [5.9 2.3]\n",
      " [5.7 2.5]\n",
      " [5.2 2.3]\n",
      " [5.  1.9]\n",
      " [5.2 2. ]\n",
      " [5.4 2.3]\n",
      " [5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Carregar dados de exemplo\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Selecionar as 2 melhores features\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_novas = selector.fit_transform(X, y)\n",
    "\n",
    "print(X_novas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset disponível no sklearn (ex: Iris).\n",
    "2. Aplique as três técnicas de seleção de features: Filtragem, Wrapper e Embedding.\n",
    "3. Compare os resultados de cada técnica e discuta as vantagens e desvantagens de cada uma.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Selecionar as features úteis para o modelo usando uma das três técnicas: Filtragem, Wrapper e Embedding\n",
    "\n",
    "H.1. Técnica de Filtragem\n",
    "\n",
    "**Definição:**\n",
    "A técnica de filtragem seleciona as features com base em estatísticas univariadas, como a correlação, variância ou outras métricas estatísticas, sem considerar um modelo de machine learning específico. \n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.7 0.4]\n",
      " [1.4 0.3]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.1]\n",
      " [1.5 0.2]\n",
      " [1.6 0.2]\n",
      " [1.4 0.1]\n",
      " [1.1 0.1]\n",
      " [1.2 0.2]\n",
      " [1.5 0.4]\n",
      " [1.3 0.4]\n",
      " [1.4 0.3]\n",
      " [1.7 0.3]\n",
      " [1.5 0.3]\n",
      " [1.7 0.2]\n",
      " [1.5 0.4]\n",
      " [1.  0.2]\n",
      " [1.7 0.5]\n",
      " [1.9 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.4]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.2]\n",
      " [1.5 0.4]\n",
      " [1.5 0.1]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.2 0.2]\n",
      " [1.3 0.2]\n",
      " [1.4 0.1]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.3 0.3]\n",
      " [1.3 0.3]\n",
      " [1.3 0.2]\n",
      " [1.6 0.6]\n",
      " [1.9 0.4]\n",
      " [1.4 0.3]\n",
      " [1.6 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [4.7 1.4]\n",
      " [4.5 1.5]\n",
      " [4.9 1.5]\n",
      " [4.  1.3]\n",
      " [4.6 1.5]\n",
      " [4.5 1.3]\n",
      " [4.7 1.6]\n",
      " [3.3 1. ]\n",
      " [4.6 1.3]\n",
      " [3.9 1.4]\n",
      " [3.5 1. ]\n",
      " [4.2 1.5]\n",
      " [4.  1. ]\n",
      " [4.7 1.4]\n",
      " [3.6 1.3]\n",
      " [4.4 1.4]\n",
      " [4.5 1.5]\n",
      " [4.1 1. ]\n",
      " [4.5 1.5]\n",
      " [3.9 1.1]\n",
      " [4.8 1.8]\n",
      " [4.  1.3]\n",
      " [4.9 1.5]\n",
      " [4.7 1.2]\n",
      " [4.3 1.3]\n",
      " [4.4 1.4]\n",
      " [4.8 1.4]\n",
      " [5.  1.7]\n",
      " [4.5 1.5]\n",
      " [3.5 1. ]\n",
      " [3.8 1.1]\n",
      " [3.7 1. ]\n",
      " [3.9 1.2]\n",
      " [5.1 1.6]\n",
      " [4.5 1.5]\n",
      " [4.5 1.6]\n",
      " [4.7 1.5]\n",
      " [4.4 1.3]\n",
      " [4.1 1.3]\n",
      " [4.  1.3]\n",
      " [4.4 1.2]\n",
      " [4.6 1.4]\n",
      " [4.  1.2]\n",
      " [3.3 1. ]\n",
      " [4.2 1.3]\n",
      " [4.2 1.2]\n",
      " [4.2 1.3]\n",
      " [4.3 1.3]\n",
      " [3.  1.1]\n",
      " [4.1 1.3]\n",
      " [6.  2.5]\n",
      " [5.1 1.9]\n",
      " [5.9 2.1]\n",
      " [5.6 1.8]\n",
      " [5.8 2.2]\n",
      " [6.6 2.1]\n",
      " [4.5 1.7]\n",
      " [6.3 1.8]\n",
      " [5.8 1.8]\n",
      " [6.1 2.5]\n",
      " [5.1 2. ]\n",
      " [5.3 1.9]\n",
      " [5.5 2.1]\n",
      " [5.  2. ]\n",
      " [5.1 2.4]\n",
      " [5.3 2.3]\n",
      " [5.5 1.8]\n",
      " [6.7 2.2]\n",
      " [6.9 2.3]\n",
      " [5.  1.5]\n",
      " [5.7 2.3]\n",
      " [4.9 2. ]\n",
      " [6.7 2. ]\n",
      " [4.9 1.8]\n",
      " [5.7 2.1]\n",
      " [6.  1.8]\n",
      " [4.8 1.8]\n",
      " [4.9 1.8]\n",
      " [5.6 2.1]\n",
      " [5.8 1.6]\n",
      " [6.1 1.9]\n",
      " [6.4 2. ]\n",
      " [5.6 2.2]\n",
      " [5.1 1.5]\n",
      " [5.6 1.4]\n",
      " [6.1 2.3]\n",
      " [5.6 2.4]\n",
      " [5.5 1.8]\n",
      " [4.8 1.8]\n",
      " [5.4 2.1]\n",
      " [5.6 2.4]\n",
      " [5.1 2.3]\n",
      " [5.1 1.9]\n",
      " [5.9 2.3]\n",
      " [5.7 2.5]\n",
      " [5.2 2.3]\n",
      " [5.  1.9]\n",
      " [5.2 2. ]\n",
      " [5.4 2.3]\n",
      " [5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Carregar dados de exemplo\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Selecionar as 2 melhores features\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_novas = selector.fit_transform(X, y)\n",
    "\n",
    "print(X_novas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset disponível no sklearn (ex: Iris).\n",
    "2. Aplique a técnica de filtragem para selecionar as melhores features.\n",
    "3. Compare os resultados com o dataset original e discuta a relevância das features selecionadas.\n",
    "\n",
    "H.2. Técnica Wrapper\n",
    "\n",
    "**Definição:**\n",
    "A técnica Wrapper utiliza um modelo de machine learning para avaliar a importância de cada subset de features. A técnica mais comum é o método de seleção recursiva de features (RFE).\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Selecionadas: [False False  True  True]\n",
      "Ranking das Features: [3 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregar dados de exemplo\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Aplicar RFE com regressão logística\n",
    "modelo = LogisticRegression(max_iter=200)\n",
    "selector = RFE(modelo, n_features_to_select=2, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "print(\"Features Selecionadas:\", selector.support_)\n",
    "print(\"Ranking das Features:\", selector.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset disponível no sklearn (ex: Iris).\n",
    "2. Aplique a técnica Wrapper usando RFE para selecionar as melhores features.\n",
    "3. Compare os resultados com a técnica de filtragem e discuta as vantagens e desvantagens.\n",
    "\n",
    "H.3. Técnica Embedding\n",
    "\n",
    "**Definição:**\n",
    "A técnica Embedding seleciona as features durante o treinamento do modelo. Modelos baseados em árvores, como Random Forest, são comuns para esta técnica.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.7 0.4]\n",
      " [1.4 0.3]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.1]\n",
      " [1.5 0.2]\n",
      " [1.6 0.2]\n",
      " [1.4 0.1]\n",
      " [1.1 0.1]\n",
      " [1.2 0.2]\n",
      " [1.5 0.4]\n",
      " [1.3 0.4]\n",
      " [1.4 0.3]\n",
      " [1.7 0.3]\n",
      " [1.5 0.3]\n",
      " [1.7 0.2]\n",
      " [1.5 0.4]\n",
      " [1.  0.2]\n",
      " [1.7 0.5]\n",
      " [1.9 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.4]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.2]\n",
      " [1.5 0.4]\n",
      " [1.5 0.1]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.2 0.2]\n",
      " [1.3 0.2]\n",
      " [1.4 0.1]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.3 0.3]\n",
      " [1.3 0.3]\n",
      " [1.3 0.2]\n",
      " [1.6 0.6]\n",
      " [1.9 0.4]\n",
      " [1.4 0.3]\n",
      " [1.6 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [4.7 1.4]\n",
      " [4.5 1.5]\n",
      " [4.9 1.5]\n",
      " [4.  1.3]\n",
      " [4.6 1.5]\n",
      " [4.5 1.3]\n",
      " [4.7 1.6]\n",
      " [3.3 1. ]\n",
      " [4.6 1.3]\n",
      " [3.9 1.4]\n",
      " [3.5 1. ]\n",
      " [4.2 1.5]\n",
      " [4.  1. ]\n",
      " [4.7 1.4]\n",
      " [3.6 1.3]\n",
      " [4.4 1.4]\n",
      " [4.5 1.5]\n",
      " [4.1 1. ]\n",
      " [4.5 1.5]\n",
      " [3.9 1.1]\n",
      " [4.8 1.8]\n",
      " [4.  1.3]\n",
      " [4.9 1.5]\n",
      " [4.7 1.2]\n",
      " [4.3 1.3]\n",
      " [4.4 1.4]\n",
      " [4.8 1.4]\n",
      " [5.  1.7]\n",
      " [4.5 1.5]\n",
      " [3.5 1. ]\n",
      " [3.8 1.1]\n",
      " [3.7 1. ]\n",
      " [3.9 1.2]\n",
      " [5.1 1.6]\n",
      " [4.5 1.5]\n",
      " [4.5 1.6]\n",
      " [4.7 1.5]\n",
      " [4.4 1.3]\n",
      " [4.1 1.3]\n",
      " [4.  1.3]\n",
      " [4.4 1.2]\n",
      " [4.6 1.4]\n",
      " [4.  1.2]\n",
      " [3.3 1. ]\n",
      " [4.2 1.3]\n",
      " [4.2 1.2]\n",
      " [4.2 1.3]\n",
      " [4.3 1.3]\n",
      " [3.  1.1]\n",
      " [4.1 1.3]\n",
      " [6.  2.5]\n",
      " [5.1 1.9]\n",
      " [5.9 2.1]\n",
      " [5.6 1.8]\n",
      " [5.8 2.2]\n",
      " [6.6 2.1]\n",
      " [4.5 1.7]\n",
      " [6.3 1.8]\n",
      " [5.8 1.8]\n",
      " [6.1 2.5]\n",
      " [5.1 2. ]\n",
      " [5.3 1.9]\n",
      " [5.5 2.1]\n",
      " [5.  2. ]\n",
      " [5.1 2.4]\n",
      " [5.3 2.3]\n",
      " [5.5 1.8]\n",
      " [6.7 2.2]\n",
      " [6.9 2.3]\n",
      " [5.  1.5]\n",
      " [5.7 2.3]\n",
      " [4.9 2. ]\n",
      " [6.7 2. ]\n",
      " [4.9 1.8]\n",
      " [5.7 2.1]\n",
      " [6.  1.8]\n",
      " [4.8 1.8]\n",
      " [4.9 1.8]\n",
      " [5.6 2.1]\n",
      " [5.8 1.6]\n",
      " [6.1 1.9]\n",
      " [6.4 2. ]\n",
      " [5.6 2.2]\n",
      " [5.1 1.5]\n",
      " [5.6 1.4]\n",
      " [6.1 2.3]\n",
      " [5.6 2.4]\n",
      " [5.5 1.8]\n",
      " [4.8 1.8]\n",
      " [5.4 2.1]\n",
      " [5.6 2.4]\n",
      " [5.1 2.3]\n",
      " [5.1 1.9]\n",
      " [5.9 2.3]\n",
      " [5.7 2.5]\n",
      " [5.2 2.3]\n",
      " [5.  1.9]\n",
      " [5.2 2. ]\n",
      " [5.4 2.3]\n",
      " [5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Carregar dados de exemplo\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Aplicar Random Forest para seleção de features\n",
    "modelo = RandomForestClassifier(n_estimators=100)\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Selecionar as features com base na importância\n",
    "selector = SelectFromModel(modelo, prefit=True)\n",
    "X_novas = selector.transform(X)\n",
    "\n",
    "print(X_novas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset disponível no sklearn (ex: Iris).\n",
    "2. Aplique a técnica Embedding usando Random Forest para selecionar as melhores features.\n",
    "3. Compare os resultados com as técnicas de Filtragem e Wrapper e discuta as diferenças.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
