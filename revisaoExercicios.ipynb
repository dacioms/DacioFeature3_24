{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisão e Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Manipular dados textuais para algoritmos de Machine Learning\n",
    "\n",
    "B.1. Utilizar a técnica de Bag-of-Words para transformar textos em vetores\n",
    "\n",
    "A técnica Bag-of-Words representa textos como a frequência de palavras, ignorando a ordem.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento' 'este' 'primeiro' 'segundo' 'terceiro']\n",
      "[[1 1 1 0 0]\n",
      " [2 1 0 1 0]\n",
      " [0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Documentos de exemplo\n",
    "documentos = [\"Este é o primeiro documento.\", \"Este documento é o segundo documento.\", \"E este é o terceiro.\"]\n",
    "\n",
    "# Aplicar Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.2. Utilizar a técnica de Bag-of-nGrams para transformar textos em vetores\n",
    "\n",
    "Bag-of-nGrams é uma extensão do Bag-of-Words que considera sequências de n palavras.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento segundo' 'este documento' 'este primeiro' 'este terceiro'\n",
      " 'primeiro documento' 'segundo documento']\n",
      "[[0 0 1 0 1 0]\n",
      " [1 1 0 0 0 1]\n",
      " [0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Documentos de exemplo\n",
    "documentos = [\"Este é o primeiro documento.\", \"Este documento é o segundo documento.\", \"E este é o terceiro.\"]\n",
    "\n",
    "# Aplicar Bag-of-nGrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.3. Remover as stopwords para ter features mais significativas\n",
    "\n",
    "Stopwords são palavras comuns que são removidas para focar nas palavras mais importantes.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento' 'este' 'primeiro' 'segundo' 'terceiro']\n",
      "[[1 1 1 0 0]\n",
      " [2 1 0 1 0]\n",
      " [0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Documentos de exemplo\n",
    "documentos = [\"Este é o primeiro documento.\", \"Este documento é o segundo documento.\", \"E este é o terceiro.\"]\n",
    "\n",
    "# Aplicar Bag-of-Words com remoção de stopwords\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.4. Realizar stemming e lemmatization de expressões usando a biblioteca nltk\n",
    "\n",
    "Stemming e lemmat\n",
    "\n",
    "ization são técnicas para reduzir palavras às suas formas base ou raiz.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'easili', 'fairli']\n",
      "['running', 'run', 'easily', 'fairly']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dacio.souza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Exemplo de frases\n",
    "frases = [\"running\", \"runs\", \"easily\", \"fairly\"]\n",
    "\n",
    "# Aplicar stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in frases]\n",
    "\n",
    "# Aplicar lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in frases]\n",
    "\n",
    "print(stemmed_words)\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.5. Calcular Tf-Idf das palavras em uma base de documentos\n",
    "\n",
    "TF-IDF é uma técnica que pondera a frequência das palavras pela inversa da frequência nos documentos.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento' 'este' 'primeiro' 'segundo' 'terceiro']\n",
      "[[0.54783215 0.42544054 0.72033345 0.         0.        ]\n",
      " [0.7948031  0.30861775 0.         0.52253528 0.        ]\n",
      " [0.         0.50854232 0.         0.         0.861037  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Documentos de exemplo\n",
    "documentos = [\"Este é o primeiro documento.\", \"Este documento é o segundo documento.\", \"E este é o terceiro.\"]\n",
    "\n",
    "# Aplicar TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.6. Criar um modelo de classificação simples usando uma base codificada por TF-IDF\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Documentos de exemplo\n",
    "documentos = [\"Este é um bom filme.\", \"Eu não gostei deste filme.\", \"Filme excelente, recomendo!\", \"Não foi um bom filme.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = positivo, 0 = negativo\n",
    "\n",
    "# Aplicar TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Treinar modelo de classificação\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar modelo\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Colete um conjunto de 50 reviews de filmes, classificando-os como positivos ou negativos.\n",
    "2. Utilize a técnica TF-IDF para transformar os textos em vetores.\n",
    "3. Treine um modelo de classificação utilizando `MultinomialNB`.\n",
    "4. Avalie a performance do modelo utilizando métricas como acurácia, precisão e recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# B. Manipular dados textuais para algoritmos de Machine Learning\n",
    "\n",
    "## B.1. Utilizar a técnica de Bag-of-Words para transformar textos em vetores\n",
    "\n",
    "**Definição:**\n",
    "A técnica Bag-of-Words representa textos como a frequência de palavras, ignorando a ordem.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento' 'este' 'primeiro' 'segundo' 'terceiro']\n",
      "[[1 1 1 0 0]\n",
      " [2 1 0 1 0]\n",
      " [0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Documentos de exemplo\n",
    "documentos = [\"Este é o primeiro documento.\", \"Este documento é o segundo documento.\", \"E este é o terceiro.\"]\n",
    "\n",
    "# Aplicar Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Colete 10 textos de notícias de diferentes fontes.\n",
    "2. Aplique a técnica de Bag-of-Words para transformar os textos em vetores.\n",
    "3. Verifique as palavras mais frequentes em cada documento e no conjunto total.\n",
    "\n",
    "B.2. Utilizar a técnica de Bag-of-nGrams para transformar textos em vetores\n",
    "\n",
    "**Definição:**\n",
    "Bag-of-nGrams é uma extensão do Bag-of-Words que considera sequências de n palavras.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento segundo' 'este documento' 'este primeiro' 'este terceiro'\n",
      " 'primeiro documento' 'segundo documento']\n",
      "[[0 0 1 0 1 0]\n",
      " [1 1 0 0 0 1]\n",
      " [0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Documentos de exemplo\n",
    "documentos = [\"Este é o primeiro documento.\", \"Este documento é o segundo documento.\", \"E este é o terceiro.\"]\n",
    "\n",
    "# Aplicar Bag-of-nGrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize os mesmos textos do exercício anterior.\n",
    "2. Aplique a técnica de Bag-of-nGrams considerando n=2.\n",
    "3. Compare os resultados com a técnica de Bag-of-Words.\n",
    "\n",
    "B.3. Remover as stopwords para ter features mais significativas\n",
    "\n",
    "**Definição:**\n",
    "Stopwords são palavras comuns que são removidas para focar nas palavras mais importantes.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento' 'este' 'primeiro' 'segundo' 'terceiro']\n",
      "[[1 1 1 0 0]\n",
      " [2 1 0 1 0]\n",
      " [0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Documentos de exemplo\n",
    "documentos = [\"Este é o primeiro documento.\", \"Este documento é o segundo documento.\", \"E este é o terceiro.\"]\n",
    "\n",
    "# Aplicar Bag-of-Words com remoção de stopwords\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize os textos do exercício anterior.\n",
    "2. Aplique a técnica de Bag-of-Words removendo stopwords.\n",
    "3. Compare os resultados antes e depois da remoção de stopwords.\n",
    "\n",
    "B.4. Realizar stemming e lemmatization de expressões usando a biblioteca nltk\n",
    "\n",
    "**Definição:**\n",
    "Stemming e lemmatization são técnicas para reduzir palavras às suas formas base ou raiz.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'easili', 'fairli']\n",
      "['running', 'run', 'easily', 'fairly']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dacio.souza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Exemplo de frases\n",
    "frases = [\"running\", \"runs\", \"easily\", \"fairly\"]\n",
    "\n",
    "# Aplicar stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in frases]\n",
    "\n",
    "# Aplicar lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in frases]\n",
    "\n",
    "print(stemmed_words)\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Colete um conjunto de 20 palavras variadas (verbos, advérbios, substantivos).\n",
    "2. Aplique o stemming utilizando o `PorterStemmer`.\n",
    "3. Aplique a lemmatization utilizando o `WordNetLemmatizer`.\n",
    "4. Compare os resultados e discuta as diferenças entre as duas técnicas.\n",
    "\n",
    "B.5. Calcular Tf-Idf das palavras em uma base de documentos\n",
    "\n",
    "**Definição:**\n",
    "TF-IDF é uma técnica que pondera a frequência das palavras pela inversa da frequência nos documentos.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento' 'este' 'primeiro' 'segundo' 'terceiro']\n",
      "[[0.54783215 0.42544054 0.72033345 0.         0.        ]\n",
      " [0.7948031  0.30861775 0.         0.52253528 0.        ]\n",
      " [0.         0.50854232 0.         0.         0.861037  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Documentos de exemplo\n",
    "documentos = [\"Este é o primeiro documento.\", \"Este documento é o segundo documento.\", \"E este é o terceiro.\"]\n",
    "\n",
    "# Aplicar TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize os textos dos exercícios anteriores.\n",
    "2. Aplique a técnica TF-IDF para calcular a importância das palavras.\n",
    "3. Compare os vetores TF-IDF com os vetores de Bag-of-Words e Bag-of-nGrams.\n",
    "\n",
    "B.6. Criar um modelo de classificação simples usando uma base codificada por TF-IDF\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Documentos de exemplo\n",
    "documentos = [\"Este é um bom filme.\", \"Eu não gostei deste filme.\", \"Filme excelente, recomendo!\", \"Não foi um bom filme.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = positivo, 0 = negativo\n",
    "\n",
    "# Aplicar TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Treinar modelo de classificação\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar modelo\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "C. Vetorizar dados categóricos para algoritmos de Machine Learning\n",
    "\n",
    "C.1. Vetorizar variáveis categóricas usando One-Hot encoding\n",
    "\n",
    "**Definição:**\n",
    "One-Hot Encoding transforma cada categoria em uma coluna binária separada.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   categoria_A  categoria_B  categoria_C\n",
      "0         True        False        False\n",
      "1        False         True        False\n",
      "2        False        False         True\n",
      "3         True        False        False\n",
      "4        False         True        False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = pd.DataFrame({'categoria': ['A', 'B', 'C', 'A', 'B']})\n",
    "\n",
    "# Aplicar One-Hot Encoding\n",
    "dados_encoded = pd.get_dummies(dados, columns=['categoria'])\n",
    "\n",
    "print(dados_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados com 3 variáveis categóricas e 100 exemplos.\n",
    "2. Aplique a técnica One-Hot Encoding.\n",
    "3. Compare a dimensão do dataset antes e depois da transformação.\n",
    "\n",
    "C.2. Vetorizar variáveis categóricas usando Dummy encoding\n",
    "\n",
    "**Definição:**\n",
    "Dummy Encoding é semelhante ao One-Hot Encoding, mas omite uma das categorias para evitar colinearidade perfeita.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   categoria_B  categoria_C\n",
      "0        False        False\n",
      "1         True        False\n",
      "2        False         True\n",
      "3        False        False\n",
      "4         True        False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = pd.DataFrame({'categoria': ['A', 'B', 'C', 'A', 'B']})\n",
    "\n",
    "# Aplicar Dummy Encoding\n",
    "dados_encoded = pd.get_dummies(dados, columns=['categoria'], drop_first=True)\n",
    "\n",
    "print(dados_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize os mesmos dados do exercício anterior.\n",
    "2. Aplique a técnica Dummy Encoding.\n",
    "3. Compare os resultados com a técnica One-Hot Encoding e discuta as diferenças.\n",
    "\n",
    "C.3. Vetorizar variáveis categóricas usando Effect encoding\n",
    "\n",
    "**Definição:**\n",
    "Effect Encoding transforma categorias em variáveis que representam o efeito de cada categoria comparado à média.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (600882661.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 7\u001b[1;36m\u001b[0m\n",
      "\u001b[1;33m    .DataFrame({'categoria': ['A', 'B', 'C', 'A', 'B']})\u001b[0m\n",
      "\u001b[1;37m    ^\u001b[0m\n",
      "\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = pd\n",
    "\n",
    ".DataFrame({'categoria': ['A', 'B', 'C', 'A', 'B']})\n",
    "dados['intercept'] = 1\n",
    "\n",
    "# Aplicar Effect Encoding\n",
    "dados_encoded = pd.get_dummies(dados, columns=['categoria'], drop_first=True)\n",
    "dados_encoded = sm.add_constant(dados_encoded, prepend=False, has_constant='add')\n",
    "\n",
    "print(dados_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados com uma variável categórica e 50 exemplos.\n",
    "2. Aplique a técnica Effect Encoding.\n",
    "3. Compare os resultados com One-Hot Encoding e Dummy Encoding.\n",
    "\n",
    "C.4. Explicar os pontos positivos e negativos de vetorizar variáveis categóricas\n",
    "\n",
    "**Definição:**\n",
    "- **Pontos Positivos:**\n",
    "  - Permite que modelos matemáticos trabalhem com dados categóricos.\n",
    "  - Melhora a interpretação do impacto de cada categoria no modelo.\n",
    "\n",
    "- **Pontos Negativos:**\n",
    "  - Aumento significativo da dimensionalidade dos dados, especialmente com muitas categorias.\n",
    "  - Pode introduzir colinearidade (ocorrência de Dummy Trap).\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Discorra sobre os pontos positivos e negativos de cada método de vetorizar variáveis categóricas.\n",
    "2. Aplique diferentes métodos de vetorizar em um conjunto de dados real e compare os resultados.\n",
    "\n",
    "C.5. Manipular grande volume de variáveis categóricas com Feature Hashing e Bin Counting\n",
    "\n",
    "**Feature Hashing:** Transforma variáveis categóricas em uma representação numérica de dimensão fixa usando uma função hash.\n",
    "**Bin Counting:** Conta a frequência de cada categoria e usa essas contagens como features.\n",
    "\n",
    "**Exemplo em Python (Feature Hashing):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = [{'categoria': 'A'}, {'categoria': 'B'}, {'categoria': 'C'}, {'categoria': 'A'}, {'categoria': 'B'}]\n",
    "\n",
    "# Aplicar Feature Hashing\n",
    "hasher = FeatureHasher(input_type='dict', n_features=5)\n",
    "dados_hashed = hasher.transform(dados)\n",
    "\n",
    "print(dados_hashed.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados com uma variável categórica contendo muitas categorias (mais de 100).\n",
    "2. Aplique a técnica Feature Hashing.\n",
    "3. Compare os resultados com a técnica One-Hot Encoding em termos de dimensionalidade e desempenho.\n",
    "\n",
    "D. Reduzir dimensionalidade através da análise de componentes principais (PCA)\n",
    "\n",
    "D.1. Explicar o que é uma projeção linear aplicada ao espaço de dados\n",
    "\n",
    "**Definição:**\n",
    "Uma projeção linear é a transformação de um espaço de alta dimensão para uma subespaço de menor dimensão usando combinações lineares das variáveis originais. Em PCA, as projeções são feitas de forma a maximizar a variância explicada pelos dados.\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados com 5 features e 200 exemplos.\n",
    "2. Realize uma projeção linear reduzindo a dimensionalidade para 2D.\n",
    "3. Plote os dados projetados em um gráfico de dispersão.\n",
    "\n",
    "D.2. Explicar as etapas do algoritmo de PCA para extração dos componentes principais\n",
    "\n",
    "**Definição:**\n",
    "1. **Centralizar os dados:** Subtrair a média de cada variável.\n",
    "2. **Calcular a matriz de covariância:** Para entender as relações entre as variáveis.\n",
    "3. **Calcular os autovalores e autovetores:** Para encontrar as direções das maiores variâncias.\n",
    "4. **Ordenar os autovetores:** De acordo com os autovalores, em ordem decrescente.\n",
    "5. **Projetar os dados originais:** Nos novos eixos (autovetores) para reduzir a dimensionalidade.\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados e aplique as etapas do PCA manualmente utilizando uma biblioteca como NumPy.\n",
    "2. Compare os resultados com a aplicação do PCA utilizando a biblioteca scikit-learn.\n",
    "\n",
    "D.3. Utilizar PCA em uma base dados e selecionar as componentes mais relevantes\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 5)  # 100 exemplos, 5 features\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "dados_reduzidos = pca.fit_transform(dados)\n",
    "\n",
    "print(dados_reduzidos)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset real e aplique PCA para reduzir a dimensionalidade.\n",
    "2. Selecione as componentes que explicam pelo menos 95% da variância.\n",
    "3. Plote a variância explicada por cada componente.\n",
    "\n",
    "D.4. Relacionar o processo de whitening e PCA\n",
    "\n",
    "**Definição:**\n",
    "O processo de whitening em PCA normaliza os dados projetados para que tenham média zero e variância unitária. Isso remove qualquer correlação entre os componentes principais, tornando-os independentes.\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Aplique PCA com whitening em um conjunto de dados.\n",
    "2. Compare os dados transformados com e sem whitening utilizando gráficos de dispersão.\n",
    "\n",
    "D.5. Escolher o número de componentes de uma PCA baseado na curva de carga\n",
    "\n",
    "**Definição:**\n",
    "A curva de carga (scree plot) mostra a variância explicada por cada componente principal. O número de componentes é escolhido com base no \"cotovelo\" da curva, onde a taxa de variação da variância explicada diminui significativamente.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 5)  # 100 exemplos, 5 features\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA()\n",
    "pca.fit(dados)\n",
    "\n",
    "# Plotar curva de carga\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Número de Componentes')\n",
    "plt.ylabel('Variância Explicada Acumulada')\n",
    "plt.title('Curva de Carga')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset real e aplique PCA.\n",
    "2. Plote a curva de carga e escolha o número de componentes baseado na curva.\n",
    "3. Justifique a escolha do número de componentes.\n",
    "\n",
    "D.6. Explicar as limitações da PCA\n",
    "\n",
    "**Definição:**\n",
    "- Assume linearidade nas relações entre variáveis.\n",
    "- Sensível à escala das variáveis.\n",
    "- Não é adequada para dados com relações não lineares.\n",
    "- Pode ser difícil interpretar os componentes principais.\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Discorra sobre as limitações da PCA.\n",
    "2. Aplique PCA em um conjunto de dados e discuta os resultados considerando as limitações.\n",
    "\n",
    "D.7. Aplicar PCA em diversas bases de diferentes contextos\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Carregar dados de exemplo (Iris dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_reduzido = pca.fit_transform(X)\n",
    "\n",
    "print(X_reduzido)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Aplique PCA em diferentes conjuntos de dados (ex: financeiro, biológico, etc.).\n",
    "2. Compare os resultados e discuta a aplicabilidade do PCA em cada contexto.\n",
    "\n",
    "**Exercícios Práticos para Alunos sobre Vetorização de Dados Textuais**\n",
    "\n",
    "### Exercício 1: Utilizar a técnica de Bag-of-Words para transformar textos em vetores\n",
    "\n",
    "**Objetivo:**\n",
    "Aplicar a técnica Bag-of-Words em um conjunto de textos e entender como transformar textos em vetores de frequência de palavras.\n",
    "\n",
    "**Instruções:**\n",
    "1. Colete 10 textos de notícias de diferentes fontes. Cada texto deve ter entre 50 e 100 palavras.\n",
    "2. Importe a biblioteca `CountVectorizer` do scikit-learn.\n",
    "3. Aplique a técnica Bag-of-Words nos textos coletados.\n",
    "4. Liste as palavras mais frequentes em cada documento e no conjunto total.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Textos de exemplo (substitua pelos textos coletados)\n",
    "documentos = [\"Texto 1...\", \"Texto 2...\", \"Texto 3...\", \"Texto 4...\", \"Texto 5...\",\n",
    "              \"Texto 6...\", \"Texto 7...\", \"Texto 8...\", \"Texto 9...\", \"Texto 10...\"]\n",
    "\n",
    "# Aplicar Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(\"Palavras:\", vectorizer.get_feature_names_out())\n",
    "print(\"Matriz de Frequência:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2: Utilizar a técnica de Bag-of-nGrams para transformar textos em vetores\n",
    "\n",
    "**Objetivo:**\n",
    "Explorar a técnica Bag-of-nGrams e compreender como a sequência de palavras pode influenciar a representação dos textos.\n",
    "\n",
    "**Instruções:**\n",
    "1. Utilize os mesmos textos do exercício anterior.\n",
    "2. Importe a biblioteca `CountVectorizer` do scikit-learn.\n",
    "3. Aplique a técnica Bag-of-nGrams considerando n=2.\n",
    "4. Compare os resultados com a técnica Bag-of-Words.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Textos de exemplo (substitua pelos textos coletados)\n",
    "documentos = [\"Texto 1...\", \"Texto 2...\", \"Texto 3...\", \"Texto 4...\", \"Texto 5...\",\n",
    "              \"Texto 6...\", \"Texto 7...\", \"Texto 8...\", \"Texto 9...\", \"Texto 10...\"]\n",
    "\n",
    "# Aplicar Bag-of-nGrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(\"nGrams:\", vectorizer.get_feature_names_out())\n",
    "print(\"Matriz de Frequência:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3: Remover as stopwords para ter features mais significativas\n",
    "\n",
    "**Objetivo:**\n",
    "Remover palavras comuns (stopwords) para focar nas palavras mais importantes e significativas dos textos.\n",
    "\n",
    "**Instruções:**\n",
    "1. Utilize os textos do exercício anterior.\n",
    "2. Importe a biblioteca `CountVectorizer` do scikit-learn.\n",
    "3. Aplique a técnica Bag-of-Words removendo as stopwords.\n",
    "4. Compare os resultados antes e depois da remoção de stopwords.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Textos de exemplo (substitua pelos textos coletados)\n",
    "documentos = [\"Texto 1...\", \"Texto 2...\", \"Texto 3...\", \"Texto 4...\", \"Texto 5...\",\n",
    "              \"Texto 6...\", \"Texto 7...\", \"Texto 8...\", \"Texto 9...\", \"Texto 10...\"]\n",
    "\n",
    "# Aplicar Bag-of-Words com remoção de stopwords\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(\"Palavras sem Stopwords:\", vectorizer.get_feature_names_out())\n",
    "print(\"Matriz de Frequência sem Stopwords:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4: Realizar stemming e lemmatization de expressões usando a biblioteca nltk\n",
    "\n",
    "**Objetivo:**\n",
    "Aplicar as técnicas de stemming e lemmatization para reduzir palavras às suas formas base ou raiz.\n",
    "\n",
    "**Instruções:**\n",
    "1. Colete um conjunto de 20 palavras variadas (verbos, advérbios, substantivos).\n",
    "2. Importe as bibliotecas `PorterStemmer` e `WordNetLemmatizer` da nltk.\n",
    "3. Aplique o stemming utilizando o `PorterStemmer`.\n",
    "4. Aplique a lemmatization utilizando o `WordNetLemmatizer`.\n",
    "5. Compare os resultados e discuta as diferenças entre as duas técnicas.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Exemplo de palavras\n",
    "palavras = [\"running\", \"runs\", \"easily\", \"fairly\", \"driving\", \"drove\", \"better\", \"good\", \"dogs\", \"flying\"]\n",
    "\n",
    "# Aplicar stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in palavras]\n",
    "\n",
    "# Aplicar lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in palavras]\n",
    "\n",
    "print(\"Stemming:\", stemmed_words)\n",
    "print(\"Lemmatization:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 5: Calcular Tf-Idf das palavras em uma base de documentos\n",
    "\n",
    "**Objetivo:**\n",
    "Aplicar a técnica TF-IDF para calcular a importância das palavras em um conjunto de textos.\n",
    "\n",
    "**Instruções:**\n",
    "1. Utilize os textos dos exercícios anteriores.\n",
    "2. Importe a biblioteca `TfidfVectorizer` do scikit-learn.\n",
    "3. Aplique a técnica TF-IDF para calcular a importância das palavras.\n",
    "4. Compare os vetores TF-IDF com os vetores de Bag-of-Words e Bag-of-nGrams.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Textos de exemplo (substitua pelos textos coletados)\n",
    "documentos = [\"Texto 1...\", \"Texto 2...\", \"Texto 3...\", \"Texto 4...\", \"Texto 5...\",\n",
    "              \"Texto 6...\", \"Texto 7...\", \"Texto 8...\", \"Texto 9...\", \"Texto 10...\"]\n",
    "\n",
    "# Aplicar TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(\"Palavras com TF-IDF:\", vectorizer.get_feature_names_out())\n",
    "print(\"Matriz TF-IDF:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 6: Criar um modelo de classificação simples usando uma base codificada por TF-IDF\n",
    "\n",
    "**Objetivo:**\n",
    "Treinar e avaliar um modelo de classificação utilizando a técnica TF-IDF para codificar os textos.\n",
    "\n",
    "**Instruções:**\n",
    "1. Colete um conjunto de 50 reviews de filmes, classificando-os como positivos ou negativos.\n",
    "2. Utilize a técnica TF-IDF para transformar os textos em vetores.\n",
    "3. Treine um modelo de classificação utilizando `MultinomialNB`.\n",
    "4. Avalie a performance do modelo utilizando métricas como acurácia, precisão e recall.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Reviews de exemplo (substitua pelos reviews coletados)\n",
    "documentos = [\"Este é um bom filme.\", \"Eu não gostei deste filme.\", \"Filme excelente, recomendo!\", \"Não foi um bom filme.\",\n",
    "              \"Ótima atuação e enredo.\", \"Muito chato e previsível.\", \"Uma obra-prima.\", \"Não vale a pena assistir.\",\n",
    "              \"Simplesmente adorável.\", \"Terrível e decepcionante.\"]\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = positivo, 0 = negativo\n",
    "\n",
    "# Aplicar TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Treinar modelo de classificação\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar modelo\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precisão:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercícios Práticos para Alunos sobre Vetorização de Dados Textuais**\n",
    "\n",
    "### Exercício 1: Utilizar a técnica de Bag-of-Words para transformar textos em vetores\n",
    "\n",
    "**Objetivo:**\n",
    "Aplicar a técnica Bag-of-Words em um conjunto de textos e entender como transformar textos em vetores de frequência de palavras.\n",
    "\n",
    "**Instruções:**\n",
    "1. Colete 10 textos de notícias de diferentes fontes. Cada texto deve ter entre 50 e 100 palavras.\n",
    "2. Importe a biblioteca `CountVectorizer` do scikit-learn.\n",
    "3. Aplique a técnica Bag-of-Words nos textos coletados.\n",
    "4. Liste as palavras mais frequentes em cada documento e no conjunto total.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Textos de exemplo (substitua pelos textos coletados)\n",
    "documentos = [\"Texto 1...\", \"Texto 2...\", \"Texto 3...\", \"Texto 4...\", \"Texto 5...\",\n",
    "              \"Texto 6...\", \"Texto 7...\", \"Texto 8...\", \"Texto 9...\", \"Texto 10...\"]\n",
    "\n",
    "# Aplicar Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(\"Palavras:\", vectorizer.get_feature_names_out())\n",
    "print(\"Matriz de Frequência:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2: Utilizar a técnica de Bag-of-nGrams para transformar textos em vetores\n",
    "\n",
    "**Objetivo:**\n",
    "Explorar a técnica Bag-of-nGrams e compreender como a sequência de palavras pode influenciar a representação dos textos.\n",
    "\n",
    "**Instruções:**\n",
    "1. Utilize os mesmos textos do exercício anterior.\n",
    "2. Importe a biblioteca `CountVectorizer` do scikit-learn.\n",
    "3. Aplique a técnica Bag-of-nGrams considerando n=2.\n",
    "4. Compare os resultados com a técnica Bag-of-Words.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Textos de exemplo (substitua pelos textos coletados)\n",
    "documentos = [\"Texto 1...\", \"Texto 2...\", \"Texto 3...\", \"Texto 4...\", \"Texto 5...\",\n",
    "              \"Texto 6...\", \"Texto 7...\", \"Texto 8...\", \"Texto 9...\", \"Texto 10...\"]\n",
    "\n",
    "# Aplicar Bag-of-nGrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(\"nGrams:\", vectorizer.get_feature_names_out())\n",
    "print(\"Matriz de Frequência:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3: Remover as stopwords para ter features mais significativas\n",
    "\n",
    "**Objetivo:**\n",
    "Remover palavras comuns (stopwords) para focar nas palavras mais importantes e significativas dos textos.\n",
    "\n",
    "**Instruções:**\n",
    "1. Utilize os textos do exercício anterior.\n",
    "2. Importe a biblioteca `CountVectorizer` do scikit-learn.\n",
    "3. Aplique a técnica Bag-of-Words removendo as stopwords.\n",
    "4. Compare os resultados antes e depois da remoção de stopwords.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Textos de exemplo (substitua pelos textos coletados)\n",
    "documentos = [\"Texto 1...\", \"Texto 2...\", \"Texto 3...\", \"Texto 4...\", \"Texto 5...\",\n",
    "              \"Texto 6...\", \"Texto 7...\", \"Texto 8...\", \"Texto 9...\", \"Texto 10...\"]\n",
    "\n",
    "# Aplicar Bag-of-Words com remoção de stopwords\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(\"Palavras sem Stopwords:\", vectorizer.get_feature_names_out())\n",
    "print(\"Matriz de Frequência sem Stopwords:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4: Realizar stemming e lemmatization de expressões usando a biblioteca nltk\n",
    "\n",
    "**Objetivo:**\n",
    "Aplicar as técnicas de stemming e lemmatization para reduzir palavras às suas formas base ou raiz.\n",
    "\n",
    "**Instruções:**\n",
    "1. Colete um conjunto de 20 palavras variadas (verbos, advérbios, substantivos).\n",
    "2. Importe as bibliotecas `PorterStemmer` e `WordNetLemmatizer` da nltk.\n",
    "3. Aplique o stemming utilizando o `PorterStemmer`.\n",
    "4. Aplique a lemmatization utilizando o `WordNetLemmatizer`.\n",
    "5. Compare os resultados e discuta as diferenças entre as duas técnicas.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Exemplo de palavras\n",
    "palavras = [\"running\", \"runs\", \"easily\", \"fairly\", \"driving\", \"drove\", \"better\", \"good\", \"dogs\", \"flying\"]\n",
    "\n",
    "# Aplicar stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in palavras]\n",
    "\n",
    "# Aplicar lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in palavras]\n",
    "\n",
    "print(\"Stemming:\", stemmed_words)\n",
    "print(\"Lemmatization:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 5: Calcular Tf-Idf das palavras em uma base de documentos\n",
    "\n",
    "**Objetivo:**\n",
    "Aplicar a técnica TF-IDF para calcular a importância das palavras em um conjunto de textos.\n",
    "\n",
    "**Instruções:**\n",
    "1. Utilize os textos dos exercícios anteriores.\n",
    "2. Importe a biblioteca `TfidfVectorizer` do scikit-learn.\n",
    "3. Aplique a técnica TF-IDF para calcular a importância das palavras.\n",
    "4. Compare os vetores TF-IDF com os vetores de Bag-of-Words e Bag-of-nGrams.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Textos de exemplo (substitua pelos textos coletados)\n",
    "documentos = [\"Texto 1...\", \"Texto 2...\", \"Texto 3...\", \"Texto 4...\", \"Texto 5...\",\n",
    "              \"Texto 6...\", \"Texto 7...\", \"Texto 8...\", \"Texto 9...\", \"Texto 10...\"]\n",
    "\n",
    "# Aplicar TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "print(\"Palavras com TF-IDF:\", vectorizer.get_feature_names_out())\n",
    "print(\"Matriz TF-IDF:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 6: Criar um modelo de classificação simples usando uma base codificada por TF-IDF\n",
    "\n",
    "**Objetivo:**\n",
    "Treinar e avaliar um modelo de classificação utilizando a técnica TF-IDF para codificar os textos.\n",
    "\n",
    "**Instruções:**\n",
    "1. Colete um conjunto de 50 reviews de filmes, classificando-os como positivos ou negativos.\n",
    "2. Utilize a técnica TF-IDF para transformar os textos em vetores.\n",
    "3. Treine um modelo de classificação utilizando `MultinomialNB`.\n",
    "4. Avalie a performance do modelo utilizando métricas como acurácia, precisão e recall.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.3333333333333333\n",
      "Precisão: 0.3333333333333333\n",
      "Recall: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.5007009 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.58899657, 0.        ,\n",
       "        0.        , 0.3894615 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.5007009 , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.50060377, 0.        , 0.        , 0.50060377,\n",
       "        0.        , 0.33101364, 0.        , 0.50060377, 0.        ,\n",
       "        0.37231379, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.64054894, 0.42354941, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.64054894, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.45862743, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.35673538, 0.53950369, 0.        , 0.        ,\n",
       "        0.40124481, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.45862743, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.57735027, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.57735027, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.57735027],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.57735027, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.57735027, 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.53051081, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.39455653, 0.        , 0.53051081, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.53051081, 0.        ],\n",
       "       [0.70710678, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.70710678, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.70710678, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.70710678, 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Reviews de exemplo (substitua pelos reviews coletados)\n",
    "documentos = [\"Este é um bom filme.\", \"Eu não gostei deste filme.\", \"Filme excelente, recomendo!\", \"Não foi um bom filme.\",\n",
    "              \"Ótima atuação e enredo.\", \"Muito chato e previsível.\", \"Uma obra-prima.\", \"Não vale a pena assistir.\",\n",
    "              \"Simplesmente adorável.\", \"Terrível e decepcionante.\"]\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = positivo, 0 = negativo\n",
    "\n",
    "# Aplicar TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Treinar modelo de classificação\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar modelo\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precisão:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuação dos Tópicos Detalhados e Exercícios Práticos\n",
    "\n",
    "G. Reduzir dimensionalidade através da análise de componentes principais (PCA)\n",
    "\n",
    "G.1. Explicar o que é uma projeção linear aplicada ao espaço de dados\n",
    "\n",
    "**Definição:**\n",
    "Uma projeção linear é a transformação de um espaço de alta dimensão para um subespaço de menor dimensão usando combinações lineares das variáveis originais. Em PCA, as projeções são feitas de forma a maximizar a variância explicada pelos dados.\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados com 5 features e 200 exemplos.\n",
    "2. Realize uma projeção linear reduzindo a dimensionalidade para 2D.\n",
    "3. Plote os dados projetados em um gráfico de dispersão.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(200, 5)  # 200 exemplos, 5 features\n",
    "\n",
    "# Matriz de projeção (para simplificação, usamos uma matriz de projeção aleatória)\n",
    "projecao = np.random.rand(5, 2)\n",
    "\n",
    "# Projeção linear\n",
    "dados_projetados = np.dot(dados, projecao)\n",
    "\n",
    "# Plotar os dados projetados\n",
    "plt.scatter(dados_projetados[:, 0], dados_projetados[:, 1])\n",
    "plt.title('Projeção Linear dos Dados')\n",
    "plt.xlabel('Componente 1')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G.2. Explicar as etapas do algoritmo de PCA para extração dos componentes principais\n",
    "\n",
    "**Definição:**\n",
    "1. **Centralizar os dados:** Subtrair a média de cada variável.\n",
    "2. **Calcular a matriz de covariância:** Para entender as relações entre as variáveis.\n",
    "3. **Calcular os autovalores e autovetores:** Para encontrar as direções das maiores variâncias.\n",
    "4. **Ordenar os autovetores:** De acordo com os autovalores, em ordem decrescente.\n",
    "5. **Projetar os dados originais:** Nos novos eixos (autovetores) para reduzir a dimensionalidade.\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados e aplique as etapas do PCA manualmente utilizando uma biblioteca como NumPy.\n",
    "2. Compare os resultados com a aplicação do PCA utilizando a biblioteca scikit-learn.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 3)  # 100 exemplos, 3 features\n",
    "\n",
    "# Centralizar os dados\n",
    "dados_centralizados = dados - np.mean(dados, axis=0)\n",
    "\n",
    "# Calcular a matriz de covariância\n",
    "matriz_covariancia = np.cov(dados_centralizados, rowvar=False)\n",
    "\n",
    "# Calcular os autovalores e autovetores\n",
    "autovalores, autovetores = np.linalg.eig(matriz_covariancia)\n",
    "\n",
    "# Ordenar os autovetores de acordo com os autovalores\n",
    "ordem = np.argsort(autovalores)[::-1]\n",
    "autovalores_ordenados = autovalores[ordem]\n",
    "autovetores_ordenados = autovetores[:, ordem]\n",
    "\n",
    "# Projetar os dados nos novos eixos\n",
    "dados_projetados = np.dot(dados_centralizados, autovetores_ordenados)\n",
    "\n",
    "print(\"Dados Projetados:\\n\", dados_projetados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G.3. Utilizar PCA em uma base dados e selecionar as componentes mais relevantes\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 5)  # 100 exemplos, 5 features\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "dados_reduzidos = pca.fit_transform(dados)\n",
    "\n",
    "print(\"Dados Reduzidos:\\n\", dados_reduzidos)\n",
    "print(\"Variância Explicada:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset real e aplique PCA para reduzir a dimensionalidade.\n",
    "2. Selecione as componentes que explicam pelo menos 95% da variância.\n",
    "3. Plote a variância explicada por cada componente.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Carregar dados de exemplo\n",
    "dados = load_iris().data\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA()\n",
    "pca.fit(dados)\n",
    "\n",
    "# Plotar a variância explicada por cada componente\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Número de Componentes')\n",
    "plt.ylabel('Variância Explicada Acumulada')\n",
    "plt.title('Curva de Carga')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G.4. Relacionar o processo de whitening e PCA\n",
    "\n",
    "**Definição:**\n",
    "O processo de whitening em PCA normaliza os dados projetados para que tenham média zero e variância unitária. Isso remove qualquer correlação entre os componentes principais, tornando-os independentes.\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Aplique PCA com whitening em um conjunto de dados.\n",
    "2. Compare os dados transformados com e sem whitening utilizando gráficos de dispersão.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 5)  # 100 exemplos, 5 features\n",
    "\n",
    "# Aplicar PCA com whitening\n",
    "pca = PCA(whiten=True)\n",
    "dados_whitened = pca.fit_transform(dados)\n",
    "\n",
    "print(\"Dados com Whitening:\\n\", dados_whitened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G.5. Escolher o número de componentes de uma PCA baseado na curva de carga\n",
    "\n",
    "**Definição:**\n",
    "A curva de carga (scree plot) mostra a variância explicada por cada componente principal. O número de componentes é escolhido com base no \"cotovelo\" da curva, onde a taxa de variação da variância explicada diminui significativamente.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 5)  # 100 exemplos, 5 features\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA()\n",
    "pca.fit(dados)\n",
    "\n",
    "# Plotar curva de carga\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Número de Componentes')\n",
    "plt.ylabel('Variância Explicada Acumulada')\n",
    "plt.title('Curva de Carga')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset real e aplique PCA.\n",
    "2. Plote a curva de carga e escolha o número de componentes baseado na curva.\n",
    "3. Justifique a escolha do número de componentes.\n",
    "\n",
    "**Código Exemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Carregar dados de exemplo\n",
    "dados = load_wine().data\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA()\n",
    "pca.fit(dados)\n",
    "\n",
    "# Plotar a curva de carga\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Número de Componentes')\n",
    "plt.ylabel('Variância Explicada Acumulada')\n",
    "plt.title('Curva de Carga')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G.6. Explicar as limitações da PCA\n",
    "\n",
    "**Definição:**\n",
    "- Assume linearidade nas relações entre variáveis.\n",
    "- Sensível à escala das variáveis.\n",
    "- Não é adequada para dados com relações não lineares.\n",
    "- Pode ser difícil interpretar os componentes principais.\n",
    "\n",
    "**Exercício Prático:**\n",
    "1. Discorra sobre as limitações da PCA.\n",
    "2. Aplique PCA em um conjunto de dados e discuta os resultados considerando as limitações.\n",
    "\n",
    "G.7. Aplicar PCA em diversas bases de diferentes contextos\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Carregar dados de exemplo (Iris dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_reduzido = pca.fit_transform(X)\n",
    "\n",
    "print(X_reduzido)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Aplique PCA em diferentes conjuntos de dados (ex: financeiro, biológico, etc.).\n",
    "2. Compare os resultados e discuta a aplicabilidade do PCA em cada contexto.\n",
    "\n",
    "\n",
    "### Continuação dos Tópicos Detalhados e Exercícios Práticos\n",
    "\n",
    "H. Selecionar as features úteis para o modelo usando uma das três técnicas: Filtragem, Wrapper e Embedding\n",
    "\n",
    "H.1. Técnica de Filtragem\n",
    "\n",
    "**Definição:**\n",
    "A técnica de filtragem seleciona as features com base em estatísticas univariadas, como a correlação, variância ou outras métricas estatísticas, sem considerar um modelo de machine learning específico. \n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Carregar dados de exemplo\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Selecionar as 2 melhores features\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_novas = selector.fit_transform(X, y)\n",
    "\n",
    "print(X_novas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset disponível no sklearn (ex: Iris).\n",
    "2. Aplique a técnica de filtragem para selecionar as melhores features.\n",
    "3. Compare os resultados com o dataset original e discuta a relevância das features selecionadas.\n",
    "\n",
    "H.2. Técnica Wrapper\n",
    "\n",
    "**Definição:**\n",
    "A técnica Wrapper utiliza um modelo de machine learning para avaliar a importância de cada subset de features. A técnica mais comum é o método de seleção recursiva de features (RFE).\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregar dados de exemplo\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Aplicar RFE com regressão logística\n",
    "modelo = LogisticRegression(max_iter=200)\n",
    "selector = RFE(modelo, n_features_to_select=2, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "print(\"Features Selecionadas:\", selector.support_)\n",
    "print(\"Ranking das Features:\", selector.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset disponível no sklearn (ex: Iris).\n",
    "2. Aplique a técnica Wrapper usando RFE para selecionar as melhores features.\n",
    "3. Compare os resultados com a técnica de filtragem e discuta as vantagens e desvantagens.\n",
    "\n",
    "H.3. Técnica Embedding\n",
    "\n",
    "**Definição:**\n",
    "A técnica Embedding seleciona as features durante o treinamento do modelo. Modelos baseados em árvores, como Random Forest, são comuns para esta técnica.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Carregar dados de exemplo\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Aplicar Random Forest para seleção de features\n",
    "modelo = RandomForestClassifier(n_estimators=100)\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Selecionar as features com base na importância\n",
    "selector = SelectFromModel(modelo, prefit=True)\n",
    "X_novas = selector.transform(X)\n",
    "\n",
    "print(X_novas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset disponível no sklearn (ex: Iris).\n",
    "2. Aplique a técnica Embedding usando Random Forest para selecionar as melhores features.\n",
    "3. Compare os resultados com as técnicas de Filtragem e Wrapper e discuta as diferenças.\n",
    "\n",
    "---\n",
    "\n",
    "I. Manipular grande volume de variáveis categóricas com Feature Hashing e Bin Counting\n",
    "\n",
    "I.1. Feature Hashing\n",
    "\n",
    "**Definição:**\n",
    "Feature Hashing transforma variáveis categóricas em uma representação numérica de dimensão fixa usando uma função hash.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = [{'categoria': 'A'}, {'categoria': 'B'}, {'categoria': 'C'}, {'categoria': 'A'}, {'categoria': 'B'}]\n",
    "\n",
    "# Aplicar Feature Hashing\n",
    "hasher = FeatureHasher(input_type='dict', n_features=5)\n",
    "dados_hashed = hasher.transform(dados)\n",
    "\n",
    "print(dados_hashed.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados com uma variável categórica contendo muitas categorias (mais de 100).\n",
    "2. Aplique a técnica Feature Hashing.\n",
    "3. Compare os resultados com a técnica One-Hot Encoding em termos de dimensionalidade e desempenho.\n",
    "\n",
    "I.2. Bin Counting\n",
    "\n",
    "**Definição:**\n",
    "Bin Counting conta a frequência de cada categoria e usa essas contagens como features.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = pd.DataFrame({'categoria': ['A', 'B', 'C', 'A', 'B', 'A', 'C', 'B', 'A', 'C']})\n",
    "\n",
    "# Contar a frequência das categorias\n",
    "contagens = dados['categoria'].value_counts()\n",
    "\n",
    "# Transformar a variável categórica em contagens\n",
    "dados['contagem'] = dados['categoria'].map(contagens)\n",
    "\n",
    "print(dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Crie um conjunto de dados com uma variável categórica contendo muitas categorias.\n",
    "2. Aplique a técnica Bin Counting.\n",
    "3. Compare os resultados com Feature Hashing e One-Hot Encoding.\n",
    "\n",
    "---\n",
    "\n",
    "J. Aplicar PCA em diversas bases de diferentes contextos\n",
    "\n",
    "J.1. Aplicar PCA em uma base de dados financeira\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Carregar dados de exemplo (substitua pelos seus dados financeiros)\n",
    "dados_financeiros = pd.DataFrame({\n",
    "    'Renda': [60000, 65000, 70000, 75000, 80000],\n",
    "    'Despesas': [40000, 42000, 45000, 48000, 50000],\n",
    "    'Investimentos': [20000, 22000, 24000, 26000, 28000]\n",
    "})\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "dados_reduzidos = pca.fit_transform(dados_financeiros)\n",
    "\n",
    "print(\"Dados Reduzidos:\\n\", dados_reduzidos)\n",
    "print(\"Variância Explicada:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset financeiro real.\n",
    "2. Aplique PCA para reduzir a dimensionalidade.\n",
    "3. Discuta os componentes principais e a variância explicada.\n",
    "\n",
    "J.2. Aplicar PCA em uma base de dados biológicos\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Carregar dados de exemplo (Wine dataset)\n",
    "dados = load_wine().data\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "dados_reduzidos = pca.fit_transform(dados)\n",
    "\n",
    "print(\"Dados Reduzidos:\\n\", dados_reduzidos)\n",
    "print(\"Variância Explicada:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Vetorizar dados categóricos para algoritmos de Machine Learning\n",
    "\n",
    "C.1. Vetorizar variáveis categóricas usando One-Hot encoding\n",
    "\n",
    "One-Hot Encoding transforma cada categoria em uma coluna binária separada.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = pd.DataFrame({'categoria': ['A', 'B', 'C', 'A', 'B']})\n",
    "\n",
    "# Aplicar One-Hot Encoding\n",
    "dados_encoded = pd.get_dummies(dados, columns=['categoria'])\n",
    "\n",
    "print(dados_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.2. Vetorizar variáveis categóricas usando Dummy encoding\n",
    "\n",
    "Dummy Encoding é semelhante ao One-Hot Encoding, mas omite uma das categorias para evitar colinearidade perfeita.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = pd.DataFrame({'categoria': ['A', 'B', 'C', 'A', 'B']})\n",
    "\n",
    "# Aplicar Dummy Encoding\n",
    "dados_encoded = pd.get_dummies(dados, columns=['categoria'], drop_first=True)\n",
    "\n",
    "print(dados_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.3. Vetorizar variáveis categóricas usando Effect encoding\n",
    "\n",
    "Effect Encoding transforma categorias em variáveis que representam o efeito de cada categoria comparado à média.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = pd.DataFrame({'categoria': ['A', 'B', 'C', 'A', 'B']})\n",
    "dados['intercept'] = 1\n",
    "\n",
    "# Aplicar Effect Encoding\n",
    "dados_encoded = pd.get_dummies(dados, columns=['categoria'], drop_first=True)\n",
    "dados_encoded = sm.add_constant(dados_encoded, prepend=False, has_constant='add')\n",
    "\n",
    "print(dados_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.4. Explicar os pontos positivos e negativos de vetorizar variáveis categóricas\n",
    "\n",
    "- **Pontos Positivos:**\n",
    "  - Permite que modelos matemáticos trabalhem com dados categóricos.\n",
    "  - Melhora a interpretação do impacto de cada categoria no modelo.\n",
    "\n",
    "- **Pontos Negativos:**\n",
    "  - Aumento significativo da dimensionalidade dos dados, especialmente com muitas categorias.\n",
    "  - Pode introduzir colinearidade (corrência de Dummy Trap).\n",
    "\n",
    "C.5. Manipular grande volume de variáveis categóricas com Feature Hashing e Bin Counting\n",
    "\n",
    "**Feature Hashing:** Transforma variáveis categóricas em uma representação numérica de dimensão fixa usando uma função hash.\n",
    "**Bin Counting:** Conta a frequência de cada categoria e usa essas contagens como features.\n",
    "\n",
    "**Exemplo em Python (Feature Hashing):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = [{'categoria': 'A'}, {'categoria': 'B'}, {'categoria': 'C'}, {'categoria': 'A'}, {'categoria': 'B'}]\n",
    "\n",
    "# Aplicar Feature Hashing\n",
    "hasher = FeatureHasher(input_type='dict', n_features=5)\n",
    "dados_hashed = hasher.transform(dados)\n",
    "\n",
    "print(dados_hashed.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Reduzir dimensionalidade através da análise de componentes principais (PCA)\n",
    "\n",
    "D.1. Explicar o que é uma projeção linear aplicada ao espaço de dados\n",
    "\n",
    "Uma projeção linear é a transformação de um espaço de alta dimensão para uma subespaço de menor dimensão usando combinações lineares das variáveis originais. Em PCA, as projeções são feitas de forma a maximizar a variância explicada pelos dados.\n",
    "\n",
    "D.2. Explicar as etapas do algoritmo de PCA para extração dos componentes principais\n",
    "\n",
    "1. **Centralizar os dados:** Subtrair a média de cada variável.\n",
    "2. **Calcular a matriz de covariância:** Para entender as relações entre as variáveis.\n",
    "3. **Calcular os autovalores e autovetores:** Para encontrar as direções das maiores variâncias.\n",
    "4. **Ordenar os autovetores:** De acordo com os autovalores, em ordem decrescente.\n",
    "5. **Projetar os dados originais:** Nos novos eixos (autovetores) para reduzir a dimensionalidade.\n",
    "\n",
    "D.3. Utilizar PCA em uma base dados e selecionar as componentes mais relevantes\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 5)  # 100 exemplos, 5 features\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "dados_reduzidos = pca.fit_transform(dados)\n",
    "\n",
    "print(dados_reduzidos)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.4. Relacionar o processo de whitening e PCA\n",
    "\n",
    "O processo de whitening em PCA normaliza os dados projetados para que tenham média zero e variância unitária. Isso remove qualquer correlação entre os componentes principais, tornando-os independentes.\n",
    "\n",
    "D.5. Escolher o número de componentes de uma PCA baseado na curva de carga\n",
    "\n",
    "A curva de carga (scree plot) mostra a variância explicada por cada componente principal. O número de componentes é escolhido com base no \"cotovelo\" da curva, onde a taxa de variação da variância explicada diminui significativamente.\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = np.random.rand(100, 5)  # 100 exemplos, 5 features\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA()\n",
    "pca.fit(dados)\n",
    "\n",
    "# Plotar curva de carga\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Número de Componentes')\n",
    "plt.ylabel('Variância Explicada Acumulada')\n",
    "plt.title('Curva de Carga')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.6. Explicar as limitações da PCA\n",
    "\n",
    "- Assume linearidade nas relações entre variáveis.\n",
    "- Sensível a escala das variáveis.\n",
    "- Não é adequada para dados com relações não lineares.\n",
    "- Pode ser difícil interpretar os componentes principais.\n",
    "\n",
    "D.7. Aplicar PCA em diversas bases de diferentes contextos\n",
    "\n",
    "Para aplicar PCA em diferentes contextos, o processo é o mesmo, mas a interpretação dos componentes principais pode variar dependendo do domínio dos dados (financeiro, biológico, etc.).\n",
    "\n",
    "**Exemplo em Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Carregar dados de exemplo (Iris dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_reduzido = pca.fit_transform(X)\n",
    "\n",
    "print(X_reduzido)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset biológico real.\n",
    "2. Aplique PCA para reduzir a dimensionalidade.\n",
    "3. Compare os componentes principais e discuta os resultados.\n",
    "\n",
    "nltk\n",
    "scikit-learn\n",
    "statsmodels\n",
    "matplotlib\n",
    "pandas\n",
    "numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício Prático:**\n",
    "1. Utilize um dataset biológico real.\n",
    "2. Aplique PCA para reduzir a dimensionalidade.\n",
    "3. Compare os componentes principais e discuta os resultados.\n",
    "\n",
    "nltk\n",
    "scikit-learn\n",
    "statsmodels\n",
    "matplotlib\n",
    "pandas\n",
    "numpy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
